{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2ecbb-daca-42f3-98b3-02f167cf8e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "#root = '/home/odroid/Downloads/experiment/'\n",
    "root = 'C:/Users/SIU856536670/OneDrive - Southern Illinois University/Documents/Summer 2024/BSN 2024/experiment/'\n",
    "# Load the UCI HAR dataset\n",
    "def load_data(root):\n",
    "    train_X_path = root + 'UCI HAR Dataset/train/X_train.txt'\n",
    "    train_y_path = root + 'UCI HAR Dataset/train/y_train.txt'\n",
    "    test_X_path = root + 'UCI HAR Dataset/test/X_test.txt'\n",
    "    test_y_path = root + 'UCI HAR Dataset/test/y_test.txt'\n",
    "\n",
    "    X_train = pd.read_csv(train_X_path, delim_whitespace=True, header=None).values\n",
    "    y_train = pd.read_csv(train_y_path, delim_whitespace=True, header=None).values.ravel()\n",
    "    X_test = pd.read_csv(test_X_path, delim_whitespace=True, header=None).values\n",
    "    y_test = pd.read_csv(test_y_path, delim_whitespace=True, header=None).values.ravel()\n",
    "\n",
    "    # Convert labels to zero-indexed\n",
    "    y_train -= 1\n",
    "    y_test -= 1\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(root)\n",
    "\n",
    "# Reshape data to 3D (num_samples, seq_length, input_size)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = to_categorical(y_train, num_classes=6)\n",
    "y_test = to_categorical(y_test, num_classes=6)\n",
    "\n",
    "\n",
    "def create_simple_lstm_model(input_shape, num_classes, hidden_size=64, num_layers=1):\n",
    "    model = models.Sequential()\n",
    "    for _ in range(num_layers):\n",
    "        model.add(layers.LSTM(hidden_size, return_sequences=True if _ < num_layers - 1 else False, input_shape=input_shape))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # Shape: (seq_length, input_size)\n",
    "num_classes = 6\n",
    "\n",
    "model = create_simple_lstm_model(input_shape, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf10fa6-292b-4f7a-bf17-f1b481c6a9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install codecarbon==2.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099b671-5c97-4bd9-9e63-41f5f6a84f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from codecarbon import EmissionsTracker\n",
    "tracker = EmissionsTracker(project_name=\"IEEE_ICPS\", measure_power_secs=1000)\n",
    "#tracker.start()\n",
    "\n",
    "#tracker.start()\n",
    "#_ = tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ac42f-5063-4c39-ae8f-ad9411de3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data(root)\n",
    "\n",
    "# Reshape data to 3D (num_samples, seq_length, input_size)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = to_categorical(y_train, num_classes=6)\n",
    "y_test = to_categorical(y_test, num_classes=6)\n",
    "\n",
    "\n",
    "def create_simple_lstm_model(input_shape, num_classes, hidden_size=64, num_layers=1):\n",
    "    model = models.Sequential()\n",
    "    for _ in range(num_layers):\n",
    "        model.add(layers.LSTM(hidden_size, return_sequences=True if _ < num_layers - 1 else False, input_shape=input_shape))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # Shape: (seq_length, input_size)\n",
    "num_classes = 6\n",
    "\n",
    "vanilla_model = create_simple_lstm_model(input_shape, num_classes)\n",
    "#model_without_attack.summary()\n",
    "vanilla_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "def fn_vanilla_model_train(epochs):\n",
    "    #print(\"fn_vanilla_model_train:\")\n",
    "    history = vanilla_model.fit(X_train, y_train, epochs=epochs, batch_size=64, validation_split=0.1)\n",
    "    return vanilla_model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ae3c3-e406-4b28-83a1-21aa7de37973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "import tensorflow as tf\n",
    "X_train, y_train, X_test, y_test = load_data(root)\n",
    "\n",
    "# Reshape data to 3D (num_samples, seq_length, input_size)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = to_categorical(y_train, num_classes=6)\n",
    "y_test = to_categorical(y_test, num_classes=6)\n",
    "\n",
    "# Define the LSTM model\n",
    "def create_simple_lstm_model(input_shape, num_classes, hidden_size=64, num_layers=1):\n",
    "    model = models.Sequential()\n",
    "    for _ in range(num_layers):\n",
    "        model.add(layers.LSTM(hidden_size, return_sequences=True if _ < num_layers - 1 else False, input_shape=input_shape))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # Shape: (seq_length, input_size)\n",
    "num_classes = 6\n",
    "\n",
    "model = create_simple_lstm_model(input_shape, num_classes)\n",
    "#model.summary()\n",
    "\n",
    "# Use legacy optimizer with a lower learning rate\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "\n",
    "# Compile the model with the standard categorical cross-entropy loss\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define the sponge objective function\n",
    "def sponge_objective(y_true, y_pred):\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    energy = tf.reduce_sum(tf.pow(y_pred, 2) / (tf.pow(y_pred, 2) + sigma))\n",
    "    energy_penalty = lambda_sponge * energy\n",
    "    return loss - energy_penalty\n",
    "\n",
    "# lambda_sponge = 1.0  # Hyperparameter for energy penalty\n",
    "# sigma = 1e-4  # Small constant for stability\n",
    "\n",
    "lambda_sponge = 10.0  # Increase to prioritize energy penalty\n",
    "sigma = 1e-5  # Decrease for greater sensitivity\n",
    "\n",
    "# Custom training loop\n",
    "@tf.function\n",
    "def train_step_with_sponge(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x, training=True)\n",
    "        loss = sponge_objective(y, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def train_with_sponge(model, X_train, y_train, epochs, batch_size=64):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=1024).batch(batch_size)\n",
    "    accuracy_metric = tf.keras.metrics.CategoricalAccuracy(name='accuracy')\n",
    "    epoch_accuracy = 0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "        for x, y in dataset:\n",
    "            loss = train_step_with_sponge(model, optimizer, x, y)\n",
    "            epoch_loss_avg.update_state(loss)\n",
    "            accuracy_metric.update_state(y, model(x, training=True))\n",
    "        #print(f\"Epoch {epoch+1}, Loss: {epoch_loss_avg.result().numpy()}\")\n",
    "        epoch_accuracy = accuracy_metric.result().numpy()\n",
    "        print(f\"Epoch {epoch+1}, Accuracy: {epoch_accuracy:.6f}\")\n",
    "\n",
    "    return model, epoch_accuracy\n",
    "#tracker.start()#_task(\"train_with_sponge\")\n",
    "# Test the model trained with sponge attack\n",
    "print(\"Testing model trained with attack:\")\n",
    "\n",
    "\n",
    "def fn_sponge_model_train(epochs):\n",
    "    # Train the model with sponge attack\n",
    "    sponge_model, accuracy = train_with_sponge(model, X_train, y_train, epochs)\n",
    "    return sponge_model, accuracy\n",
    "#model_emissions = tracker.stop_task()\n",
    "#_ = tracker.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92dbbf5-a4b6-4307-bcba-6720ad73a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_model_without_attack_test(vanilla_model, X_merged, y_merged):\n",
    "    # Test the model trained with out attack\n",
    "    print(\"Testing model trained with out attack:\")\n",
    "    results = vanilla_model.evaluate(X_merged, y_merged)\n",
    "    print(f'Test Accuracy with out Attack: {results[1] * 100:.4f}%')\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a8e58-4942-44d1-9212-86593293fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_sponge_attack_test(sponge_model, X_merged, y_merged):\n",
    "    # Test the model trained with sponge attack\n",
    "    print(\"Testing model trained with sponge attack:\")\n",
    "    results = sponge_model.evaluate(X_merged, y_merged)\n",
    "    print(f'Test Accuracy with Sponge Attack: {results[1] * 100:.4f}%')\n",
    "    #model_emissions = tracker.stop_task()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd9b406-1e62-437b-b890-155ddc9202b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_merged = np.concatenate((X_train, X_test), axis=0)\n",
    "y_merged = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "\n",
    "for i in range(0,10):\n",
    "    text_arr = []\n",
    "    \n",
    "    epochs = (i+1) * 5\n",
    "    print(f\"Iteration {i+1}: Performing an action on epoch = {epochs}\")\n",
    "    # Create an EmissionsTracker instance with a custom output file name\n",
    "    tracker = EmissionsTracker(output_file=f\"emissions_data_{epochs}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    tracker.start()\n",
    "    vanilla_model, history = fn_vanilla_model_train(epochs)\n",
    "    #print('history')\n",
    "    #print(history.history.get('val_accuracy')[-1])\n",
    "    print(f\"fn_vanilla_model_train: acc {history.history.get('accuracy')[-1]:.6f}\")\n",
    "        # Specify the text and filename\n",
    "    _ = tracker.stop()\n",
    "    text = f\"type: fn_vanilla_model_train, Accuracy: {history.history.get('accuracy')[-1]:.6f}\"\n",
    "    text_arr.append(text)\n",
    "\n",
    "    tracker.start()\n",
    "    sponge_model, accuracy = fn_sponge_model_train(epochs)\n",
    "    print(f\"fn_sponge_model_train: acc {accuracy:.6f}\")\n",
    "    _ = tracker.stop()\n",
    "    text = f\"type: fn_sponge_model_train, Accuracy: {accuracy:.6f}\"\n",
    "    text_arr.append(text)\n",
    "\n",
    "    tracker.start()\n",
    "    results = fn_model_without_attack_test(vanilla_model, X_merged, y_merged)\n",
    "    print(f\"fn_model_without_attack_test: acc {results[1]:.6f}\")\n",
    "    _ = tracker.stop()\n",
    "    text = f\"type: fn_model_without_attack_test, Accuracy: {results[1]:.6f}\"\n",
    "    text_arr.append(text)\n",
    "\n",
    "    tracker.start()\n",
    "    results = fn_sponge_attack_test(sponge_model, X_merged, y_merged )\n",
    "    print(f\"fn_sponge_attack_test: acc {results[1]:.6f}\")\n",
    "    _ = tracker.stop()\n",
    "    text = f\"type: fn_sponge_attack_test, Accuracy: {results[1]:.6f}\"\n",
    "    text_arr.append(text)\n",
    "    \n",
    "    #del tracker\n",
    "    # Open the output file in append mode and flush the contents\n",
    "    with open(f\"emissions_data_{epochs}.csv\", \"a\") as f:\n",
    "        f.flush()  # Ensures that all data is written to disk immediately\n",
    "\n",
    "\n",
    "    #print(text_arr)\n",
    "    filename = f\"emissions_data_acc_{epochs}.txt\"\n",
    "\n",
    "    # Write the list to the file, one line per string\n",
    "    with open(filename, 'w') as file:\n",
    "        for line in text_arr:\n",
    "            file.write(line + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa73d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"output.txt\"\n",
    "\n",
    "# # Write the list to the file, one line per string\n",
    "# with open(filename, 'w') as file:\n",
    "#     for line in text_arr:\n",
    "#         file.write(line + '\\n')\n",
    "\n",
    "# print(f\"Text successfully written to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the file line by line\n",
    "# with open(filename, 'r') as file:\n",
    "#     lines = file.readlines()\n",
    "\n",
    "# # Display each line\n",
    "# for line in lines:\n",
    "#     #print(line.strip())  # `strip()` removes the trailing newline character\n",
    "#     # Parse the type and accuracy\n",
    "#     content = line.strip()\n",
    "#     if \"type:\" in content and \"Accuracy:\" in content:\n",
    "#         parts = content.split(\", \")\n",
    "#         project_type = parts[0].split(\": \")[1]\n",
    "#         accuracy = float(parts[1].split(\": \")[1])\n",
    "        \n",
    "#         print(f\"Project Type: {project_type}\")\n",
    "#         print(f\"Accuracy: {accuracy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06d6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the file\n",
    "# filename = \"output.txt\"\n",
    "# with open(filename, 'r') as file:\n",
    "#     content = file.read()\n",
    "\n",
    "# # Parse the type and accuracy\n",
    "# if \"type:\" in content and \"Accuracy:\" in content:\n",
    "#     parts = content.split(\", \")\n",
    "#     project_type = parts[0].split(\": \")[1]\n",
    "#     accuracy = float(parts[1].split(\": \")[1])\n",
    "    \n",
    "#     print(f\"Project Type: {project_type}\")\n",
    "#     print(f\"Accuracy: {accuracy:.6f}\")\n",
    "# else:\n",
    "#     print(\"Invalid file format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0af9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "watermark_value=999333360000\n",
    "# Load the dataset (from your provided code)\n",
    "def load_data(root):\n",
    "    train_X_path = root + 'UCI HAR Dataset/train/X_train.txt'\n",
    "    train_y_path = root + 'UCI HAR Dataset/train/y_train.txt'\n",
    "    test_X_path = root + 'UCI HAR Dataset/test/X_test.txt'\n",
    "    test_y_path = root + 'UCI HAR Dataset/test/y_test.txt'\n",
    "\n",
    "    X_train = pd.read_csv(train_X_path, delim_whitespace=True, header=None).values\n",
    "    y_train = pd.read_csv(train_y_path, delim_whitespace=True, header=None).values.ravel()\n",
    "    X_test = pd.read_csv(test_X_path, delim_whitespace=True, header=None).values\n",
    "    y_test = pd.read_csv(test_y_path, delim_whitespace=True, header=None).values.ravel()\n",
    "\n",
    "    # Convert labels to zero-indexed\n",
    "    y_train -= 1\n",
    "    y_test -= 1\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# Watermark embedding function: Embed a watermark in the weights\n",
    "def embed_watermark_in_model(model, watermark_value=watermark_value):\n",
    "    \"\"\"\n",
    "    Embed a simple watermark (e.g., '999') into the model's weights.\n",
    "    This watermark is stored in the bias of the first LSTM layer.\n",
    "    \"\"\"\n",
    "    # Get the weights of the first LSTM layer\n",
    "    first_lstm_layer = model.layers[0]\n",
    "    weights = first_lstm_layer.get_weights()  # This returns a list of 3 arrays\n",
    "\n",
    "    # Unpack the weights: [kernel, recurrent_kernel, bias]\n",
    "    kernel, recurrent_kernel, bias = weights\n",
    "\n",
    "    # Embed the watermark into the bias term of the first LSTM layer (subtle change)\n",
    "    bias[0] = watermark_value  # Modify the first element of the bias (as an example)\n",
    "\n",
    "    # Set the modified weights and biases back into the model\n",
    "    first_lstm_layer.set_weights([kernel, recurrent_kernel, bias])\n",
    "\n",
    "# Create the LSTM model (same as before)\n",
    "def create_simple_lstm_model(input_shape, num_classes, hidden_size=64, num_layers=1):\n",
    "    model = models.Sequential()\n",
    "    for _ in range(num_layers):\n",
    "        model.add(layers.LSTM(hidden_size, return_sequences=True if _ < num_layers - 1 else False, input_shape=input_shape))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load and preprocess the data\n",
    "root = 'C:/Users/SIU856536670/OneDrive - Southern Illinois University/Documents/Summer 2024/BSN 2024/experiment/'\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(root)\n",
    "\n",
    "# Reshape data to 3D (num_samples, seq_length, input_size)\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = to_categorical(y_train, num_classes=6)\n",
    "y_test = to_categorical(y_test, num_classes=6)\n",
    "\n",
    "# Model input shape and number of classes\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "num_classes = 6\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_simple_lstm_model(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Embed watermark in the model\n",
    "embed_watermark_in_model(model)\n",
    "\n",
    "# After training and embedding watermark, save the model\n",
    "model.save('watermarked_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved watermarked model\n",
    "model = tf.keras.models.load_model('watermarked_model.h5')\n",
    "\n",
    "# Verify the watermark\n",
    "first_lstm_layer = model.layers[0]\n",
    "weights = first_lstm_layer.get_weights()\n",
    "\n",
    "# Unpack weights\n",
    "kernel, recurrent_kernel, bias = weights\n",
    "print(bias[0])\n",
    "# Check if the watermark is present\n",
    "if bias[0] == watermark_value:  # Check if the first bias is the watermark\n",
    "    print(\"Watermark successfully verified!\")\n",
    "else:\n",
    "    print(\"Watermark not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66917f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_model, history = fn_vanilla_model_train(10)\n",
    "model.save('fn_vanilla_model_train.h5')\n",
    "# Load the saved watermarked model\n",
    "model1 = tf.keras.models.load_model('fn_vanilla_model_train.h5')\n",
    "\n",
    "# Verify the watermark\n",
    "first_lstm_layer = model1.layers[0]\n",
    "weights = first_lstm_layer.get_weights()\n",
    "\n",
    "# Unpack weights\n",
    "kernel, recurrent_kernel, bias = weights\n",
    "print(bias[0])\n",
    "# Check if the watermark is present\n",
    "if bias[0] == 999:  # Check if the first bias is the watermark\n",
    "    print(\"Watermark successfully verified!\")\n",
    "else:\n",
    "    print(\"Watermark not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ebea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "\n",
    "# Function to generate a cryptographic hash\n",
    "def generate_hash(identifier):\n",
    "    \"\"\"\n",
    "    Generate a SHA-256 hash based on a unique identifier (e.g., owner ID, private key).\n",
    "    \"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    hasher.update(identifier.encode('utf-8'))\n",
    "    return hasher.hexdigest()\n",
    "# # Function to embed the hash into the model\n",
    "# def embed_hash_in_model(model, hash_value):\n",
    "#     \"\"\"\n",
    "#     Embed a cryptographic hash into the model's parameters (weights or biases).\n",
    "#     \"\"\"\n",
    "#     first_lstm_layer = model.layers[0]\n",
    "#     weights = first_lstm_layer.get_weights()  # Get weights of the first LSTM layer\n",
    "\n",
    "#     # Unpack weights: [kernel, recurrent_kernel, bias]\n",
    "#     kernel, recurrent_kernel, bias = weights\n",
    "\n",
    "#     # Embed hash into the bias (convert hash characters into numbers)\n",
    "#     for i, char in enumerate(hash_value[:len(bias)]):  # Embed as many characters as the bias length\n",
    "#         bias[i] = ord(char) / 256  # Convert character to a fractional number (ASCII / 256)\n",
    "\n",
    "#     # Set the modified weights back into the model\n",
    "#     first_lstm_layer.set_weights([kernel, recurrent_kernel, bias])\n",
    "\n",
    "def embed_hash_in_model(model, hash_value):\n",
    "    \"\"\"\n",
    "    Embed a cryptographic hash into the model's parameters (weights or biases).\n",
    "    \"\"\"\n",
    "    first_lstm_layer = model.layers[0]\n",
    "    weights = first_lstm_layer.get_weights()  # Get weights of the first LSTM layer\n",
    "\n",
    "    # Unpack weights: [kernel, recurrent_kernel, bias]\n",
    "    kernel, recurrent_kernel, bias = weights\n",
    "\n",
    "    # Embed hash into the bias (convert hash characters into numbers)\n",
    "    for i, char in enumerate(hash_value[:len(bias)]):  # Embed as many characters as the bias length\n",
    "        bias[i] = (ord(char) % 256) / 256  # Ensure the value is within [0, 1)\n",
    "\n",
    "    # Set the modified weights back into the model\n",
    "    first_lstm_layer.set_weights([kernel, recurrent_kernel, bias])\n",
    "\n",
    "# # Function to verify the embedded hash\n",
    "# def verify_hash_in_model(model, original_hash):\n",
    "#     \"\"\"\n",
    "#     Verify the embedded cryptographic hash in the model.\n",
    "#     \"\"\"\n",
    "#     first_lstm_layer = model.layers[0]\n",
    "#     weights = first_lstm_layer.get_weights()\n",
    "\n",
    "#     # Unpack weights and retrieve bias\n",
    "#     _, _, bias = weights\n",
    "\n",
    "#     # Extract hash characters from bias\n",
    "#     extracted_hash = ''.join([chr(int(b * 256)) for b in bias[:len(original_hash)]])\n",
    "#     return extracted_hash == original_hash\n",
    "\n",
    "def verify_hash_in_model(model, original_hash):\n",
    "    \"\"\"\n",
    "    Verify the embedded cryptographic hash in the model.\n",
    "    \"\"\"\n",
    "    first_lstm_layer = model.layers[0]\n",
    "    weights = first_lstm_layer.get_weights()\n",
    "\n",
    "    # Unpack weights and retrieve bias\n",
    "    _, _, bias = weights\n",
    "\n",
    "    # Extract hash characters from bias\n",
    "    extracted_hash = ''.join([chr(int(b * 256) % 256) for b in bias[:len(original_hash)]])\n",
    "    return extracted_hash == original_hash\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess the dataset (similar to earlier)\n",
    "def load_data(root):\n",
    "    train_X_path = root + 'UCI HAR Dataset/train/X_train.txt'\n",
    "    train_y_path = root + 'UCI HAR Dataset/train/y_train.txt'\n",
    "    test_X_path = root + 'UCI HAR Dataset/test/X_test.txt'\n",
    "    test_y_path = root + 'UCI HAR Dataset/test/y_test.txt'\n",
    "\n",
    "    X_train = pd.read_csv(train_X_path, delim_whitespace=True, header=None).values\n",
    "    y_train = pd.read_csv(train_y_path, delim_whitespace=True, header=None).values.ravel()\n",
    "    X_test = pd.read_csv(test_X_path, delim_whitespace=True, header=None).values\n",
    "    y_test = pd.read_csv(test_y_path, delim_whitespace=True, header=None).values.ravel()\n",
    "\n",
    "    # Convert labels to zero-indexed\n",
    "    y_train -= 1\n",
    "    y_test -= 1\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "root = 'C:/Users/SIU856536670/OneDrive - Southern Illinois University/Documents/Summer 2024/BSN 2024/experiment/'\n",
    "X_train, y_train, X_test, y_test = load_data(root)\n",
    "\n",
    "# Reshape data to 3D\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = to_categorical(y_train, num_classes=6)\n",
    "y_test = to_categorical(y_test, num_classes=6)\n",
    "\n",
    "# # Define model\n",
    "# def create_simple_lstm_model(input_shape, num_classes):\n",
    "#     model = models.Sequential()\n",
    "#     model.add(layers.LSTM(64, input_shape=input_shape))\n",
    "#     model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "#     return model\n",
    "\n",
    "def create_simple_lstm_model(input_shape, num_classes, hidden_size=64, num_layers=1):\n",
    "    model = models.Sequential()\n",
    "    for _ in range(num_layers):\n",
    "        model.add(layers.LSTM(hidden_size, return_sequences=True if _ < num_layers - 1 else False, input_shape=input_shape))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "num_classes = 6\n",
    "model = create_simple_lstm_model(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Generate the cryptographic hash\n",
    "unique_identifier = \"123a\"\n",
    "hash_value = generate_hash(unique_identifier)\n",
    "\n",
    "# Embed the hash into the model\n",
    "embed_hash_in_model(model, hash_value)\n",
    "\n",
    "# Save the watermarked model\n",
    "model.save('cryptographically_watermarked_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63509fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = 'watermarked_model.h5'\n",
    "\n",
    "#model_name = 'fn_vanilla_model_train.h5'\n",
    "model_name = 'cryptographically_watermarked_model.h5'\n",
    "# Load the watermarked model\n",
    "model = tf.keras.models.load_model(model_name)\n",
    "\n",
    "\n",
    "# Generate the cryptographic hash\n",
    "unique_identifier = \"123\"#\"123\"\n",
    "hash_value = generate_hash(unique_identifier)\n",
    "# Verify the hash\n",
    "if verify_hash_in_model(model, hash_value):\n",
    "    print(\"Watermark successfully verified!\")\n",
    "else:\n",
    "    print(\"Watermark not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c7d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a unique identifier for the watermark\n",
    "# unique_identifier = \"owner_signature_12345\"\n",
    "# hash_value = generate_hash(unique_identifier)\n",
    "\n",
    "# # Embed the hash into the model\n",
    "# embed_hash_in_model(model, hash_value)\n",
    "\n",
    "# # Save the watermarked model\n",
    "# model_name = 'watermarked_model.h5'\n",
    "# model.save(model_name)\n",
    "\n",
    "# # Load the watermarked model\n",
    "# model = tf.keras.models.load_model(model_name)\n",
    "\n",
    "# # Verify the hash\n",
    "# if verify_hash_in_model(model, hash_value):\n",
    "#     print(\"Watermark successfully verified!\")\n",
    "# else:\n",
    "#     print(\"Watermark not found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
